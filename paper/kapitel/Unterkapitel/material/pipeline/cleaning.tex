\subsubsection{Data Cleaning}
\label{subsubsec:cleaning}
The quality of data can be described in many dimensions like \eg{} the trustworthiness of the data source, the consistency of the data,
or their accuracy as well as their topicality.
Just with very unstructured data, as in our case the collected dishes from the web page \url{speisekarte.de}, the cleaning of the data is
inevitable to get a good and accurate analysis result.
However, even with supposedly good data sources such as \acp{API}, there may be errors in the data.
such as \eg{} the mixing of different languages or the lack of information.
Differences in data quality can also occur in the process of data collection.
\newline
This paragraph explains the measures taken in this project to clean up the data collected.
\paragraph{Review Count Cleaning}
In the \pg{} database the review\_count of some restaurants was assigned the value \code{NULL}.
If a new comparison with the \ylp{} \ac{API} did not result in a change of the review count,
this \code{NULL} value was replaced by the number 0.
\paragraph{Address Cleaning}
The \ac{API} of \ylp{} is designed so that an HTTP code 403 or 404 will be transmitted if no address to a restaurant
can be found.
With a python script the \ylp{} \ac{API} was addressed again to find the restaurant's address.
If still no city or postcode was found for this restaurant, this restaurant was deleted for reasons of simplicity,
since a restaurant without a postcode or city has no use for the subsequent analysis.
\paragraph{Price Range Cleaning}
In the price\_range attribute there were occasional occuring \code{NULL} values which possibly could be filled when addressing the \ylp{} \ac{API} a second time.
If this was not the case, the \code{NULL} values were filled with the mode of the price\_range from the current city.
\paragraph{City Cleaning}
Despite the fact that the \code{city} attribute of a restaurant always came from Yelp it came to a mixture of the German and English language,
so we translated the english names in its german counterparts manually.
Furthermore, the German city ''Frankfurt am Main'' was available in different spellings.
However, this could be fixed with a simple \ac{SQL} statement.
\paragraph{Buying Power Cleaning}
The data source found for acquiring the buying\_power for various german cities unfortunately contained not the buying\_power for all german cities.
We solved this isssue by replacing, the missing buying\_power with the average buying\_power in Germany in order to be able to carry out at least a rough analysis.
\paragraph{Average Purchase Price Cleaning}
Also with the average purchase price of a restaurant per m\textsuperscript{2} the original data source did not include all cities of Germany and therefore
some \code{NULL} values remained in the database.
These have been replaced with the Germany-wide average.
\paragraph{Menu Item Cleaning}
Although most of the cleaning took place after the persistence of the data in the database, the various dishes on a menu had to be
cleaned before it was written to the database.
By scraping these data directly from the web, it often happened that they were not only semantically wrong, but also containing only
special characters or an empty string.
Especially these entries without any informational content had to be removed before they were stored in the database.
This was done with several algorithms from the area of the \textit{Text Preprocessing}\footnote{Please rfer to \fullref{subsec:review} to learn more about \textit{Text Preprocessing}}
